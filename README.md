# Ответы на экзаменационные вопросы по численным методам

## Оглавление

1. [Интерполяционная формула Лагранжа. Погрешность.](#1-интерполяционная-формула-лагранжа-погрешность)
2. [Интерполяционные формулы Ньютона. Погрешность.](#2-интерполяционные-формулы-ньютона-погрешность)
3. [Оптимальный выбор узлов интерполяции. Многочлен Чебышева.](#3-оптимальный-выбор-узлов-интерполяции-многочлен-чебышева)
4. [Сплайн-интерполяция. Обратное интерполирование.](#4-сплайн-интерполяция-обратное-интерполирование)
5. [Квадратурные формулы Ньютона-Котеса.](#5-квадратурные-формулы-ньютона-котеса)
6. [Формулы левых, правых, средних прямоугольников.](#6-формулы-левых-правых-средних-прямоугольников-геометрический-смысл-погрешности)
7. [Формула трапеции. Геометрический смысл. Погрешность.](#7-формула-трапеции-геометрический-смысл-погрешность)
8. [Формула Симпсона. Геометрический смысл. Погрешность.](#8-формула-симпсона-геометрический-смысл-погрешность)
9. [Формула 3/8. Погрешность.](#9-формула-38-погрешность)
10. [Нормы векторов и матриц. Согласованность норм.](#10-нормы-векторов-и-матриц-определения-и-свойства-согласованность-норм)
11. [Устойчивость СЛАУ. Обусловленность СЛАУ.](#11-устойчивость-слау-обусловленность-слау)
12. [Метод простых итераций. Сходимость.](#12-метод-простых-итераций-сходимость)
13. [Метод Зейделя. Сходимость.](#13-метод-зейделя-случай-нормальной-системы-сходимость)
14. [Каноническая форма одношаговых итерационных методов.](#14-каноническая-форма-записи-одношаговых-итерационных-методов-примеры)
15. [Вариационные методы решения СЛАУ.](#15-вариационные-методы-решения-слау-выбор-оптимального-параметра)
16. [Метод прогонки решения СЛАУ.](#16-метод-прогонки-решения-слау-условия-устойчивости)
17. [Обращение матриц. Метод окаймления.](#17-обращение-матриц-метод-окаймления)
18. [LU-разложение.](#18-метод-разложения-в-произведение-двух-треугольных-матриц)
19. [Метод главных элементов.](#19-решение-слау-методом-главных-элементов)
20. [Метод квадратных корней.](#20-метод-квадратных-корней)
21. [Методы отделения корней нелинейных уравнений.](#21-методы-отделения-корней-нелинейных-уравнений)
22. [Метод половинного деления.](#22-метод-половинного-деления)
23. [Метод хорд. Геометрический смысл.](#23-метод-хорд-геометрический-смысл)
24. [Метод касательных (Ньютона).](#24-метод-касательных-ньютона-геометрический-смысл-особенности)
25. [Метод итерации для нелинейных уравнений.](#25-метод-итерации-решения-нелинейных-уравнений)
26. [Метод итерации для систем нелинейных уравнений.](#26-метод-итерации-решения-систем-нелинейных-уравнений)
27. [Метод Ньютона для систем нелинейных уравнений.](#27-метод-ньютона-решения-систем-нелинейных-уравнений)
28. [Задача на собственные числа. Метод векового определителя.](#28-постановка-задачи-на-собственные-числа-метод-развертывания-векового-определителя)
29. [Метод Данилевского.](#29-метод-данилевского-исключительные-случаи)
30. [Метод Крылова.](#30-метод-крылова)
31. [Степенной метод (метод итераций).](#31-частичная-проблема-метод-итераций-степенной-метод)
32. [Метод скалярных произведений.](#32-частичная-проблема-метод-скалярных-произведений)
33. [Постановки задач для ОДУ. Классификация методов.](#33-постановки-задач-для-оду-классификация-методов)
34. [Задача Коши. Метод рядов. Метод Пикара.](#34-задача-коши-метод-разложения-в-ряд-метод-последовательных-приближений)
35. [Метод Эйлера.](#35-метод-эйлера-усовершенствованный-метод-ломаных-порядок-точности)
36. [Метод Эйлера-Коши.](#36-метод-эйлера-коши-итерационная-обработка-порядок-точности)
37. [Метод Рунге-Кутта.](#37-метод-рунге-кутта-достоинства-порядок-точности)
38. [Многошаговые методы. Метод Адамса и Милна.](#38-многошаговые-методы-метод-адамса-и-милна-порядок-точности)

---

## 1. Интерполяционная формула Лагранжа. Погрешность.

**Что это:** Метод построения интерполяционного многочлена, проходящего через заданный набор точек.

**Для чего используется:** Восстановление значений функции между узлами таблицы, аппроксимация функций многочленами, экстраполяция данных.

**Формула Лагранжа** — способ построения многочлена степени n, проходящего через (n+1) точек.

$$L_n(x) = \sum_{i=0}^{n} y_i \cdot \prod_{j=0, j \neq i}^{n} \frac{x - x_j}{x_i - x_j}$$

**Погрешность:**
$$R_n(x) = \frac{f^{(n+1)}(\xi)}{(n+1)!} \prod_{i=0}^{n}(x - x_i)$$

где $\xi \in [x_0, x_n]$.

**Достоинства:** универсальность, не требует равноотстоящих узлов.
**Недостатки:** при добавлении нового узла нужно пересчитывать всё заново.

---

## 2. Интерполяционные формулы Ньютона. Погрешность.

**Что это:** Альтернативная форма записи интерполяционного многочлена через конечные разности.

**Для чего используется:** Интерполяция табличных функций с равноотстоящими узлами, вычисление значений в начале (первая формула) или конце (вторая формула) таблицы.

**Первая формула Ньютона** (для равноотстоящих узлов, интерполяция вперёд):
$$N_n(x) = y_0 + \binom{q}{1}\Delta y_0 + \binom{q}{2}\Delta^2 y_0 + ... + \binom{q}{n}\Delta^n y_0$$

где $q = \frac{x - x_0}{h}$, $\Delta y_i = y_{i+1} - y_i$ — конечные разности.

**Вторая формула Ньютона** (интерполяция назад):
$$N_n(x) = y_n + q\nabla y_n + \frac{q(q+1)}{2!}\nabla^2 y_n + ...$$

где $q = \frac{x - x_n}{h}$.

**Погрешность:** аналогична формуле Лагранжа.

**Преимущество перед Лагранжем:** легко добавлять новые узлы, не пересчитывая всё.

---

## 3. Оптимальный выбор узлов интерполяции. Многочлен Чебышева.

**Что это:** Способ выбора узлов интерполяции, минимизирующий максимальную погрешность на отрезке.

**Для чего используется:** Уменьшение погрешности интерполяции, борьба с эффектом Рунге (осцилляции на краях), оптимальная аппроксимация функций.

**Проблема:** минимизировать максимальную погрешность интерполяции.

**Многочлен Чебышева:**
$$T_n(x) = \cos(n \cdot \arccos(x)), \quad x \in [-1, 1]$$

**Узлы Чебышева** (корни многочлена):
$$x_k = \cos\frac{(2k+1)\pi}{2n}, \quad k = 0, 1, ..., n-1$$

**Свойство:** узлы Чебышева минимизируют $\max|ω_n(x)|$, где $ω_n(x) = \prod(x - x_i)$.

**Важно:** узлы сгущаются к краям отрезка, что компенсирует рост погрешности на границах.

---

## 4. Сплайн-интерполяция. Обратное интерполирование.

**Что это:** Сплайн — кусочно-полиномиальная функция, «склеенная» в узлах с условиями гладкости.

**Для чего используется:** Гладкая интерполяция данных (компьютерная графика, CAD-системы), избежание осцилляций при большом числе узлов, построение кривых по контрольным точкам.

**Кубический сплайн** на $[x_i, x_{i+1}]$:
$$S_i(x) = a_i + b_i(x-x_i) + c_i(x-x_i)^2 + d_i(x-x_i)^3$$

**Условия:**
- Непрерывность сплайна и его 1-й и 2-й производных
- $S(x_i) = y_i$

**Обратное интерполирование** — нахождение x по заданному y. 

**Применение:** когда известна таблица $y = f(x)$, но нужно найти $x$ для заданного $y$.

**Методы:**
1. Построить интерполяцию $x = \phi(y)$ (поменять местами x и y)
2. Решить уравнение $L_n(x) = y$ численно

---

## 5. Квадратурные формулы Ньютона-Котеса.

**Что это:** Семейство формул численного интегрирования, основанных на замене подынтегральной функции интерполяционным многочленом.

**Для чего используется:** Приближённое вычисление определённых интегралов, когда первообразная неизвестна или сложно вычисляется, обработка экспериментальных данных.

**Общий вид:**
$$\int_a^b f(x)dx \approx (b-a)\sum_{i=0}^{n} H_i f(x_i)$$

где $H_i$ — коэффициенты Котеса (зависят только от n, не от функции).

**Примеры:**
- n=1: формула трапеций
- n=2: формула Симпсона
- n=3: формула 3/8

**Принцип:** чем выше n, тем точнее формула (до определённого предела).

---

## 6. Формулы левых, правых, средних прямоугольников. Геометрический смысл. Погрешности.

**Что это:** Простейшие квадратурные формулы для численного интегрирования.

**Для чего используется:** Быстрая грубая оценка интеграла, базовые методы для понимания численного интегрирования, основа для более сложных методов.

**Левых прямоугольников:**
$$\int_a^b f(x)dx \approx h\sum_{i=0}^{n-1} f(x_i)$$

**Правых прямоугольников:**
$$\int_a^b f(x)dx \approx h\sum_{i=1}^{n} f(x_i)$$

**Средних прямоугольников:**
$$\int_a^b f(x)dx \approx h\sum_{i=0}^{n-1} f\left(\frac{x_i + x_{i+1}}{2}\right)$$

**Геометрический смысл:** площадь под кривой заменяется суммой площадей прямоугольников с высотой, равной значению функции в левом/правом/среднем узле.

**Погрешность:**
- Левых/правых: $O(h)$ — первый порядок
- Средних: $O(h^2)$ — второй порядок (точнее!)

---

## 7. Формула трапеции. Геометрический смысл. Погрешность.

**Что это:** Квадратурная формула, аппроксимирующая подынтегральную функцию линейными функциями на каждом отрезке.

**Для чего используется:** Численное интегрирование с умеренной точностью, простота реализации, основа для метода Ромберга.

**Формула:**
$$\int_a^b f(x)dx \approx \frac{h}{2}\left[f(x_0) + 2\sum_{i=1}^{n-1}f(x_i) + f(x_n)\right]$$

**Геометрический смысл:** площадь под кривой заменяется суммой площадей трапеций, образованных соседними точками.

**Погрешность:**
$$R = -\frac{(b-a)h^2}{12}f''(\xi) = O(h^2)$$

**Свойство:** точна для линейных функций.

---

## 8. Формула Симпсона. Геометрический смысл. Погрешность.

**Что это:** Квадратурная формула высокого порядка, аппроксимирующая функцию параболами.

**Для чего используется:** Точное численное интегрирование, один из самых популярных методов на практике, хорошее соотношение точности и сложности.

**Формула (парабол):**
$$\int_a^b f(x)dx \approx \frac{h}{3}\left[f(x_0) + 4\sum_{i=1,3,5...}f(x_i) + 2\sum_{i=2,4,6...}f(x_i) + f(x_n)\right]$$

**Геометрический смысл:** кривая аппроксимируется параболами на каждой паре отрезков (через 3 точки проводится парабола).

**Погрешность:**
$$R = -\frac{(b-a)h^4}{180}f^{(4)}(\xi) = O(h^4)$$

**Важно:** требует чётного числа отрезков (n — чётное).

---

## 9. Формула 3/8. Погрешность.

**Что это:** Квадратурная формула Ньютона-Котеса для 4 узлов (n=3), аппроксимация кубической параболой.

**Для чего используется:** Интегрирование когда число отрезков кратно 3, альтернатива формуле Симпсона.

**Формула:**
$$\int_a^b f(x)dx \approx \frac{3h}{8}\left[f(x_0) + 3f(x_1) + 3f(x_2) + f(x_3)\right]$$

Применяется на 3 отрезках (4 узла), аппроксимация кубической параболой.

**Погрешность:**
$$R = -\frac{3h^5}{80}f^{(4)}(\xi) = O(h^4)$$

**Сравнение с Симпсоном:** тот же порядок точности, но немного менее эффективна.

---

## 10. Нормы векторов и матриц, определения и свойства. Согласованность норм.

**Что это:** Способ измерения «величины» вектора или матрицы, обобщение понятия длины.

**Для чего используется:** Оценка погрешностей вычислений, анализ сходимости итерационных методов, исследование устойчивости алгоритмов, сравнение решений.

**Норма вектора** — функция $\|x\|: \mathbb{R}^n \to \mathbb{R}$:
1. $\|x\| \geq 0$, $\|x\| = 0 \Leftrightarrow x = 0$ (положительная определённость)
2. $\|\alpha x\| = |\alpha| \cdot \|x\|$ (однородность)
3. $\|x + y\| \leq \|x\| + \|y\|$ (неравенство треугольника)

**Примеры:**
- $\|x\|_1 = \sum|x_i|$ — манхэттенская норма
- $\|x\|_2 = \sqrt{\sum x_i^2}$ — евклидова норма
- $\|x\|_\infty = \max|x_i|$ — чебышёвская норма

**Норма матрицы:**
- $\|A\|_1 = \max_j \sum_i |a_{ij}|$ (максимум сумм по столбцам)
- $\|A\|_\infty = \max_i \sum_j |a_{ij}|$ (максимум сумм по строкам)

**Согласованность:** $\|Ax\| \leq \|A\| \cdot \|x\|$ — важно для оценки погрешностей.

---

## 11. Устойчивость СЛАУ. Обусловленность СЛАУ.

**Что это:** Характеристики чувствительности решения системы к погрешностям входных данных.

**Для чего используется:** Оценка надёжности численного решения, выбор метода решения, понимание границ точности вычислений.

**Число обусловленности:**
$$\text{cond}(A) = \|A\| \cdot \|A^{-1}\|$$

**Свойства:**
- $\text{cond}(A) \geq 1$
- Чем больше cond(A), тем хуже обусловлена система
- cond(A) ≈ 1 — хорошо обусловленная система
- cond(A) >> 1 — плохо обусловленная система

**Оценка погрешности решения:**
$$\frac{\|\delta x\|}{\|x\|} \leq \text{cond}(A) \cdot \frac{\|\delta b\|}{\|b\|}$$

**Устойчивость:** система устойчива, если малые изменения входных данных вызывают малые изменения решения.

**Практический смысл:** при cond(A) ~ $10^k$ теряется примерно k значащих цифр.

---

## 12. Метод простых итераций. Сходимость.

**Что это:** Итерационный метод решения СЛАУ, основанный на последовательных приближениях.

**Для чего используется:** Решение больших разреженных систем, когда прямые методы неэффективны, параллельные вычисления.

**Схема:** $Ax = b$ преобразуется к виду $x = Bx + c$

**Итерационный процесс:**
$$x^{(k+1)} = Bx^{(k)} + c$$

**Условие сходимости:**
$$\|B\| < 1$$

или $\rho(B) < 1$, где $\rho(B)$ — спектральный радиус (максимальный модуль собственных чисел).

**Скорость сходимости:** $\|x^{(k)} - x^*\| \leq \|B\|^k \|x^{(0)} - x^*\|$

**Достоинства:** простота, малый объём памяти.
**Недостатки:** медленная сходимость, не всегда сходится.

---

## 13. Метод Зейделя. Случай нормальной системы. Сходимость.

**Что это:** Улучшение метода итераций за счёт немедленного использования вычисленных компонент.

**Для чего используется:** Ускорение сходимости по сравнению с простыми итерациями, решение систем с диагональным преобладанием.

**Метод Зейделя** — модификация метода итераций: при вычислении $x_i^{(k+1)}$ используются уже найденные $x_1^{(k+1)}, ..., x_{i-1}^{(k+1)}$.

$$x_i^{(k+1)} = \frac{1}{a_{ii}}\left(b_i - \sum_{j<i}a_{ij}x_j^{(k+1)} - \sum_{j>i}a_{ij}x_j^{(k)}\right)$$

**Сходимость гарантирована:**
- Для матриц с диагональным преобладанием: $|a_{ii}| > \sum_{j \neq i}|a_{ij}|$
- Для симметричных положительно определённых матриц

**Преимущество:** обычно сходится быстрее метода простых итераций.

---

## 14. Каноническая форма записи одношаговых итерационных методов. Примеры.

**Что это:** Унифицированная форма записи итерационных методов с параметром τ.

**Для чего используется:** Единообразный анализ и сравнение методов, оптимизация параметров, теоретическое исследование сходимости.

**Каноническая форма:**
$$\frac{x^{(k+1)} - x^{(k)}}{\tau} + Ax^{(k)} = b$$

или $x^{(k+1)} = x^{(k)} - \tau(Ax^{(k)} - b)$

**Интерпретация:** на каждой итерации делаем шаг размера τ в направлении уменьшения невязки.

**Примеры:**
- $\tau = 1$, простейший метод итераций
- Метод Ричардсона: $x^{(k+1)} = x^{(k)} + \tau(b - Ax^{(k)})$

**Ключевой вопрос:** выбор оптимального τ для максимальной скорости сходимости.

---

## 15. Вариационные методы решения СЛАУ. Выбор оптимального параметра.

**Что это:** Методы, сводящие решение СЛАУ к минимизации функционала.

**Для чего используется:** Решение симметричных положительно определённых систем, задачи оптимизации, метод конечных элементов.

**Идея:** минимизация функционала $F(x) = (Ax, x) - 2(b, x)$

Минимум F(x) достигается при $Ax = b$ (для симметричной положительно определённой A).

**Метод наискорейшего спуска:**
$$x^{(k+1)} = x^{(k)} - \alpha_k r^{(k)}$$

где $r^{(k)} = Ax^{(k)} - b$ — невязка (направление градиента).

**Оптимальный параметр:**
$$\alpha_k = \frac{(r^{(k)}, r^{(k)})}{(Ar^{(k)}, r^{(k)})}$$

Выбирается так, чтобы минимизировать F вдоль направления спуска.

---

## 16. Метод прогонки решения СЛАУ. Условия устойчивости.

**Что это:** Эффективный прямой метод для систем с трёхдиагональной матрицей.

**Для чего используется:** Решение разностных уравнений, сплайн-интерполяция, краевые задачи для ОДУ, численное решение уравнений в частных производных.

**Применяется** для трёхдиагональных матриц вида:
$$a_ix_{i-1} + b_ix_i + c_ix_{i+1} = d_i$$

**Прямой ход:** вычисление прогоночных коэффициентов:
$$\alpha_{i+1} = \frac{-c_i}{b_i + a_i\alpha_i}, \quad \beta_{i+1} = \frac{d_i - a_i\beta_i}{b_i + a_i\alpha_i}$$

**Обратный ход:**
$$x_i = \alpha_{i+1}x_{i+1} + \beta_{i+1}$$

**Условия устойчивости:**
$$|b_i| \geq |a_i| + |c_i|, \quad |b_i| > 0$$

**Сложность:** O(n) — очень эффективно!

---

## 17. Обращение матриц. Метод окаймления.

**Что это:** Рекурсивный метод вычисления обратной матрицы путём последовательного увеличения размерности.

**Для чего используется:** Когда нужно пересчитывать обратную матрицу при добавлении строки/столбца, задачи оптимизации с изменяющимися ограничениями.

**Метод окаймления** — последовательное наращивание размерности матрицы.

Если известна $A_k^{-1}$, то $A_{k+1}^{-1}$ вычисляется по формулам:

$$A_{k+1} = \begin{pmatrix} A_k & u \\ v^T & a_{k+1,k+1} \end{pmatrix}$$

$$A_{k+1}^{-1} = \begin{pmatrix} A_k^{-1} + \frac{A_k^{-1}uv^TA_k^{-1}}{\gamma} & -\frac{A_k^{-1}u}{\gamma} \\ -\frac{v^TA_k^{-1}}{\gamma} & \frac{1}{\gamma} \end{pmatrix}$$

где $\gamma = a_{k+1,k+1} - v^TA_k^{-1}u$.

**Начало:** $A_1^{-1} = 1/a_{11}$.

---

## 18. Метод разложения в произведение двух треугольных матриц.

**Что это:** Представление матрицы в виде произведения нижней и верхней треугольных матриц (LU-разложение).

**Для чего используется:** Эффективное решение СЛАУ, вычисление определителя, решение нескольких систем с одной матрицей, но разными правыми частями.

**LU-разложение:** $A = LU$

- $L$ — нижняя треугольная ($l_{ii} = 1$)
- $U$ — верхняя треугольная

**Формулы:**
$$u_{ij} = a_{ij} - \sum_{k=1}^{i-1}l_{ik}u_{kj}$$
$$l_{ij} = \frac{1}{u_{jj}}\left(a_{ij} - \sum_{k=1}^{j-1}l_{ik}u_{kj}\right)$$

**Решение СЛАУ:** $Ax = b \Rightarrow Ly = b, \; Ux = y$

Два простых последовательных решения треугольных систем.

**Сложность:** $O(n^3)$ для разложения, $O(n^2)$ для каждой новой правой части.

---

## 19. Решение СЛАУ методом главных элементов.

**Что это:** Модификация метода Гаусса с перестановками для повышения численной устойчивости.

**Для чего используется:** Устойчивое решение плохо обусловленных систем, уменьшение ошибок округления, стандартный метод в библиотеках линейной алгебры.

**Метод Гаусса с выбором главного элемента:**

1. На каждом шаге выбирается максимальный по модулю элемент (в столбце, строке или всей матрице)
2. Производится перестановка строк/столбцов
3. Выполняется исключение (обнуление поддиагональных элементов)

**Варианты:**
- Частичный выбор — по столбцу (наиболее используемый)
- Полный выбор — по всей подматрице (более устойчивый, но дороже)

**Цель:** повышение устойчивости вычислений, уменьшение ошибок округления, избежание деления на малые числа.

---

## 20. Метод квадратных корней.

**Что это:** Специальное LU-разложение для симметричных положительно определённых матриц (разложение Холецкого).

**Для чего используется:** Решение нормальных уравнений МНК, симметричные системы в физике и механике, вдвое экономнее обычного LU.

**Применяется** для симметричных положительно определённых матриц.

**Разложение Холецкого:** $A = LL^T$, где $L$ — нижняя треугольная.

**Формулы:**
$$l_{ii} = \sqrt{a_{ii} - \sum_{k=1}^{i-1}l_{ik}^2}$$
$$l_{ij} = \frac{1}{l_{jj}}\left(a_{ij} - \sum_{k=1}^{j-1}l_{ik}l_{jk}\right), \quad i > j$$

**Решение:** $Ly = b$, затем $L^Tx = y$.

**Преимущество:** нужно хранить только L (вдвое меньше памяти), меньше операций.

---

## 21. Методы отделения корней нелинейных уравнений.

**Что это:** Предварительный этап решения — локализация корней на интервалах.

**Для чего используется:** Определение начальных приближений для итерационных методов, гарантия нахождения всех корней, оценка числа решений.

**Задача:** найти интервалы, содержащие ровно один корень.

**Методы:**
1. **Графический** — построение графика $f(x)$, визуальный поиск пересечений с осью x
2. **Табличный** — вычисление $f(x)$ в узлах сетки, поиск смены знака
3. **Аналитический** — исследование производной, оценка числа корней

**Теорема:** если $f(a) \cdot f(b) < 0$ и $f'(x)$ сохраняет знак на $[a,b]$, то на $[a,b]$ ровно один корень.

**Важность:** без отделения можно пропустить корни или зациклиться.

---

## 22. Метод половинного деления.

**Что это:** Простейший метод уточнения корня путём последовательного деления интервала пополам.

**Для чего используется:** Гарантированное нахождение корня, начальное приближение для более быстрых методов, когда надёжность важнее скорости.

**Алгоритм (бисекции):**
1. Дано $[a, b]$, где $f(a) \cdot f(b) < 0$
2. $c = (a + b)/2$
3. Если $f(a) \cdot f(c) < 0$, то $b = c$, иначе $a = c$
4. Повторять до $|b - a| < \varepsilon$

**Скорость сходимости:** линейная, $n \approx \log_2\frac{b-a}{\varepsilon}$ итераций.

**Достоинства:** гарантированная сходимость, не требует производных.
**Недостатки:** медленная сходимость, нужен интервал со сменой знака.

---

## 23. Метод хорд. Геометрический смысл.

**Что это:** Метод уточнения корня, использующий линейную интерполяцию между точками.

**Для чего используется:** Уточнение корня без вычисления производной, компромисс между простотой и скоростью.

**Формула:**
$$x_{n+1} = x_n - \frac{f(x_n)(x_n - x_{n-1})}{f(x_n) - f(x_{n-1})}$$

или с фиксированным концом:
$$x_{n+1} = x_n - \frac{f(x_n)(b - x_n)}{f(b) - f(x_n)}$$

**Геометрический смысл:** проводим хорду через две точки графика, корень аппроксимируется точкой пересечения хорды с осью x.

**Сходимость:** сверхлинейная, порядок $\approx 1.618$ (золотое сечение).

**Преимущество:** не требует вычисления производной.

---

## 24. Метод касательных (Ньютона). Геометрический смысл. Особенности.

**Что это:** Быстросходящийся метод, использующий линеаризацию функции в текущей точке.

**Для чего используется:** Быстрое уточнение корня при хорошем начальном приближении, основа многих оптимизационных алгоритмов.

**Формула:**
$$x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}$$

**Геометрический смысл:** в точке $(x_n, f(x_n))$ проводим касательную к графику, следующее приближение — точка пересечения касательной с осью x.

**Сходимость:** квадратичная ($p = 2$) вблизи корня — число верных цифр удваивается на каждой итерации.

**Особенности:**
- Требуется вычисление производной
- Нужно хорошее начальное приближение
- Условие: $f(x_0) \cdot f''(x_0) > 0$ (начинаем с выпуклой стороны)
- Может расходиться при плохом старте или $f'(x) \approx 0$

---

## 25. Метод итерации решения нелинейных уравнений.

**Что это:** Метод последовательных приближений для уравнения, приведённого к виду x = φ(x).

**Для чего используется:** Простая реализация, когда уравнение естественно имеет форму x = φ(x), теоретическое обоснование других методов.

**Схема:** $f(x) = 0$ преобразуется к виду $x = \phi(x)$

Например: $x^3 - x - 1 = 0 \Rightarrow x = \sqrt[3]{x + 1}$

**Итерации:**
$$x_{n+1} = \phi(x_n)$$

**Условие сходимости:**
$$|\phi'(x)| \leq q < 1$$

в окрестности корня (принцип сжимающих отображений).

**Скорость сходимости:** линейная, зависит от q.

**Важно:** разные преобразования дают разную скорость (или расходимость!).

---

## 26. Метод итерации решения систем нелинейных уравнений.

**Что это:** Обобщение метода простых итераций на системы уравнений.

**Для чего используется:** Решение систем нелинейных уравнений, когда система естественно записывается в форме x = Φ(x).

**Система:** $F(x) = 0$ преобразуется к $x = \Phi(x)$

где $x = (x_1, ..., x_n)^T$, $\Phi = (\phi_1, ..., \phi_n)^T$.

**Итерации:**
$$x^{(k+1)} = \Phi(x^{(k)})$$

**Условие сходимости:**
$$\left\|\frac{\partial \Phi}{\partial x}\right\| \leq q < 1$$

в окрестности решения (матрица Якоби отображения Φ).

**Геометрический смысл:** сжимающее отображение притягивает к неподвижной точке.

---

## 27. Метод Ньютона решения систем нелинейных уравнений.

**Что это:** Обобщение метода касательных на системы уравнений.

**Для чего используется:** Быстрое решение систем нелинейных уравнений, оптимизация, обратные задачи.

**Система:** $F(x) = 0$, где $F: \mathbb{R}^n \to \mathbb{R}^n$

**Итерации:**
$$x^{(k+1)} = x^{(k)} - J^{-1}(x^{(k)}) \cdot F(x^{(k)})$$

где $J$ — матрица Якоби: $J_{ij} = \frac{\partial F_i}{\partial x_j}$

**На практике:** решается СЛАУ $J(x^{(k)}) \cdot \Delta x = -F(x^{(k)})$, затем $x^{(k+1)} = x^{(k)} + \Delta x$

**Сходимость:** квадратичная вблизи решения.

**Сложность:** на каждой итерации нужно вычислять и обращать матрицу Якоби.

---

## 28. Постановка задачи на собственные числа. Метод развертывания векового определителя.

**Что это:** Задача нахождения собственных чисел и векторов матрицы — фундаментальная задача линейной алгебры.

**Для чего используется:** Анализ устойчивости систем, колебания механических систем, главные компоненты в статистике, квантовая механика.

**Задача:** найти $\lambda$ (собственное число) и $x \neq 0$ (собственный вектор): $Ax = \lambda x$

**Характеристическое уравнение:**
$$\det(A - \lambda E) = 0$$

**Метод развертывания:** прямое вычисление определителя как многочлена от $\lambda$.

Для матрицы n×n получаем многочлен степени n:
$$(-1)^n\lambda^n + c_{n-1}\lambda^{n-1} + ... + c_0 = 0$$

**Недостаток:** вычислительно сложен для больших n, неустойчив.

---

## 29. Метод Данилевского. Исключительные случаи.

**Что это:** Метод приведения матрицы к специальной форме для нахождения характеристического многочлена.

**Для чего используется:** Нахождение всех собственных чисел матрицы, получение коэффициентов характеристического многочлена.

**Идея:** приведение матрицы к форме Фробениуса подобными преобразованиями $P = M^{-1}AM$.

**Форма Фробениуса:**
$$P = \begin{pmatrix} -p_1 & -p_2 & ... & -p_n \\ 1 & 0 & ... & 0 \\ 0 & 1 & ... & 0 \\ ... & ... & ... & ... \end{pmatrix}$$

**Характеристический многочлен:** $\lambda^n + p_1\lambda^{n-1} + ... + p_n = 0$

Коэффициенты читаются прямо из первой строки!

**Исключительный случай:** $a_{n,n-1} = 0$ — требуется перестановка строк/столбцов или переход к блочной форме.

---

## 30. Метод Крылова.

**Что это:** Метод нахождения характеристического многочлена через степени матрицы.

**Для чего используется:** Нахождение всех собственных чисел, построение минимального многочлена, основа современных методов Крылова (GMRES, CG).

**Идея:** представление характеристического многочлена через степени матрицы.

**Строится последовательность:** $y_0, y_1 = Ay_0, y_2 = A^2y_0, ..., y_n = A^ny_0$

**Система для коэффициентов:**
$$y_n + p_1y_{n-1} + ... + p_ny_0 = 0$$

Это СЛАУ относительно $p_1, ..., p_n$.

**Собственные векторы:** $x = y_0 + p_1Ay_0 + ... + p_{n-1}A^{n-1}y_0$ (после нахождения $\lambda$).

**Выбор y₀:** произвольный, но результат зависит от выбора.

---

## 31. Частичная проблема. Метод итераций (степенной метод).

**Что это:** Метод нахождения наибольшего по модулю собственного числа и соответствующего вектора.

**Для чего используется:** PageRank Google, анализ главных компонент, доминирующая мода колебаний, когда нужно только одно собственное число.

**Задача:** найти $|\lambda_1| = \max|\lambda_i|$.

**Алгоритм:**
$$y^{(k+1)} = Ay^{(k)}, \quad \lambda \approx \frac{y_i^{(k+1)}}{y_i^{(k)}}$$

**Нормировка:** $y^{(k)} := y^{(k)} / \|y^{(k)}\|$ (чтобы избежать переполнения)

**Сходимость:** $\left|\frac{\lambda_2}{\lambda_1}\right|^k$, где $|\lambda_1| > |\lambda_2|$.

**Условие:** $|\lambda_1| > |\lambda_2|$ (наибольшее собственное число по модулю единственно).

---

## 32. Частичная проблема. Метод скалярных произведений.

**Что это:** Улучшенная версия степенного метода для более точной оценки собственного числа.

**Для чего используется:** Более точное вычисление главного собственного числа, особенно для симметричных матриц.

**Улучшение степенного метода:**

$$\lambda_1 \approx \frac{(Ay^{(k)}, y^{(k)})}{(y^{(k)}, y^{(k)})}$$

Это отношение Рэлея.

**Преимущество:** более точная оценка собственного числа, для симметричных матриц погрешность $O(|\lambda_2/\lambda_1|^{2k})$ вместо $O(|\lambda_2/\lambda_1|^k)$.

**Для симметричных матриц:** все собственные числа вещественны, собственные векторы ортогональны.

---

## 33. Постановки задач для ОДУ. Классификация методов.

**Что это:** Обзор типов задач и методов для обыкновенных дифференциальных уравнений.

**Для чего используется:** Моделирование динамических систем (физика, химия, биология, экономика), управление, прогнозирование.

**Задача Коши (начальная):** $y' = f(x, y)$, $y(x_0) = y_0$

Найти y(x) при заданном начальном условии.

**Краевая задача:** условия на концах интервала, например $y(a) = A$, $y(b) = B$.

**Классификация методов:**
1. **Одношаговые:** используют информацию только в одной точке (Эйлер, Рунге-Кутта)
2. **Многошаговые:** используют несколько предыдущих точек (Адамс, Милн)
3. **Явные/неявные:** явные — прямое вычисление, неявные — решение уравнения

**Характеристики:** порядок точности, устойчивость, трудоёмкость.

---

## 34. Задача Коши. Метод разложения в ряд. Метод последовательных приближений.

**Что это:** Аналитические и полуаналитические методы решения задачи Коши.

**Для чего используется:** Получение аналитического решения вблизи начальной точки, теоретическое обоснование существования решения, высокая точность на малом интервале.

**Метод рядов Тейлора:**
$$y(x) = y_0 + y'_0(x-x_0) + \frac{y''_0}{2!}(x-x_0)^2 + ...$$

где производные вычисляются последовательным дифференцированием уравнения.

**Метод Пикара (последовательных приближений):**
$$y_{n+1}(x) = y_0 + \int_{x_0}^{x} f(t, y_n(t))dt$$

Последовательность $y_n(x) \to y(x)$ при условиях теоремы существования (Липшиц по y).

**Использование:** доказательство теоремы существования и единственности, начальное приближение.

---

## 35. Метод Эйлера. Усовершенствованный метод ломаных. Порядок точности.

**Что это:** Простейший численный метод решения задачи Коши.

**Для чего используется:** Обучение, быстрые грубые оценки, основа для понимания более сложных методов.

**Метод Эйлера (явный):**
$$y_{n+1} = y_n + hf(x_n, y_n)$$

Геометрический смысл: движемся по касательной к решению.

**Порядок точности:** 1 (локальная погрешность $O(h^2)$, глобальная $O(h)$).

**Усовершенствованный (модифицированный Эйлер):**
$$y_{n+1} = y_n + hf\left(x_n + \frac{h}{2}, y_n + \frac{h}{2}f(x_n, y_n)\right)$$

Сначала делаем полшага, вычисляем наклон в середине.

**Порядок точности:** 2.

---

## 36. Метод Эйлера-Коши. Итерационная обработка. Порядок точности.

**Что это:** Метод предиктор-корректор — комбинация явного и неявного методов.

**Для чего используется:** Повышение точности без сложных вычислений, контроль погрешности по разности предиктора и корректора.

**Метод Эйлера-Коши (предиктор-корректор):**

Предиктор (Эйлер): $\tilde{y}_{n+1} = y_n + hf(x_n, y_n)$

Корректор (трапеция): $y_{n+1} = y_n + \frac{h}{2}[f(x_n, y_n) + f(x_{n+1}, \tilde{y}_{n+1})]$

**С итерациями (уточнение корректора):**
$$y_{n+1}^{(k+1)} = y_n + \frac{h}{2}[f(x_n, y_n) + f(x_{n+1}, y_{n+1}^{(k)})]$$

Повторяем до сходимости.

**Порядок точности:** 2.

---

## 37. Метод Рунге-Кутта. Достоинства. Порядок точности.

**Что это:** Семейство одношаговых методов высокого порядка точности.

**Для чего используется:** Стандартный метод для решения ОДУ в большинстве приложений, оптимальное соотношение точности и простоты.

**Классический метод 4-го порядка (RK4):**
$$y_{n+1} = y_n + \frac{h}{6}(k_1 + 2k_2 + 2k_3 + k_4)$$

где:
- $k_1 = f(x_n, y_n)$ — наклон в начале
- $k_2 = f(x_n + h/2, y_n + hk_1/2)$ — наклон в середине (по k₁)
- $k_3 = f(x_n + h/2, y_n + hk_2/2)$ — наклон в середине (по k₂)
- $k_4 = f(x_n + h, y_n + hk_3)$ — наклон в конце

**Достоинства:**
- Высокая точность ($O(h^4)$)
- Одношаговый (самостартующий, не нужна история)
- Легко менять шаг
- Хорошая устойчивость

**Недостаток:** 4 вычисления f на шаг.

---

## 38. Многошаговые методы. Метод Адамса и Милна. Порядок точности.

**Что это:** Методы, использующие информацию из нескольких предыдущих точек для повышения эффективности.

**Для чего используется:** Когда f(x,y) дорого вычислять, долгое интегрирование (астрономия, климат), использование уже вычисленных значений.

**Метод Адамса (экстраполяционный, явный):**
$$y_{n+1} = y_n + \frac{h}{24}(55f_n - 59f_{n-1} + 37f_{n-2} - 9f_{n-3})$$

Использует 4 последних значения f (не нужны новые вычисления f!).

**Порядок:** 4.

**Метод Милна (предиктор-корректор):**

Предиктор: $\tilde{y}_{n+1} = y_{n-3} + \frac{4h}{3}(2f_n - f_{n-1} + 2f_{n-2})$

Корректор: $y_{n+1} = y_{n-1} + \frac{h}{3}(f_{n-1} + 4f_n + f_{n+1})$

**Порядок:** 4.

**Недостатки многошаговых методов:**
- Требуют «разгон» (начальные значения из одношагового метода)
- Сложнее менять шаг
- Проблемы с устойчивостью (Милн может быть неустойчив)
